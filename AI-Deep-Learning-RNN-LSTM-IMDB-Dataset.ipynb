{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook and dataset backgrounder\n",
    "I use non-client/customer specific applications of my projects due to privacy, but I would still like to showcase what I have built, so I use public datasets. This project is cross-published on Github and Kaggle.\n",
    "\n",
    "The Internet Movie Database also known as IMDB is the best place on the Internet to find information about Hollywood productions, TV shows and movies.\n",
    "\n",
    "A dataset with [25000 movie reviews from IMDB](https://keras.io/api/datasets/imdb/) was compiled by the [Keras team](https://keras.io/). It is great for demonstrating how [Long Short-Term Memory (LSTM)](https://en.wikipedia.org/wiki/Long_short-term_memory), a type of [Recurrent Neural Network (RNN)](https://en.wikipedia.org/wiki/Recurrent_neural_network) can be used.\n",
    "\n",
    "### Theory backgrounder\n",
    "This uses a neural network, a network of processing nodes (perceptrons/neurons) which activate or not dependent upon the mathematical activation equation. These mathematical neurons mimic how we think as humans with our biological neurons. The defacto type of RNN, LSTM is used in this project. As noted by a very good [WikiPedia article](https://en.wikipedia.org/wiki/Recurrent_neural_network), early RNNs had a vanishing gradient problem, so there is nothing to train against.\n",
    "\n",
    "> Unlike feedforward neural networks, which process data in a single pass, RNNs process data across multiple time steps, making them well-adapted for modelling and processing text, speech, and time series.\n",
    "\n",
    "### Tech backgrounder\n",
    "During 2022 and 2023, when I originally learned how to work with Tensorflow and Keras, I learned about various ways to use Keras, and this project is to highlight my use of an RNN, LSTM type. Now in 2024, Keras can be run on top of JAX, Tensorflow, or PyTorch.\n",
    "\n",
    "Here I will use Tensorflow and Keras, to supplt the dataset and use the LSTM.\n",
    "\n",
    "### Misc\n",
    "While the dataset as provided has 25000 reviews in total, I will use 20000 samples, limit each review to a maximum of 150 words, training the model with 15000 samples, and validating the model with 5000 samples. After the model is run, I will be able to determine the number of trainiable parameters are in the model with this build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "n_data_samples = 20000\n",
    "review_length = 150\n",
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=n_data_samples)   # REF (Keras IMDB dataset): https://keras.io/api/datasets/imdb/\n",
    "from keras.preprocessing.sequence import pad_sequences   # REF (properly importing pad_sequences, working like this 2024): https://stackoverflow.com/questions/72326025/cannot-import-name-pad-sequences-from-keras-preprocessing-sequence and https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences\n",
    "x_train = pad_sequences(x_train, maxlen=review_length)   # REF (using sequence.pad_sequences to pad/trim review length): https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences\n",
    "x_test = pad_sequences(x_test, maxlen=review_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Embedding, Embedding, LSTM, Dense\n",
    "def build_model():\n",
    "  model = Sequential()\n",
    "  model.add(Input((review_length,)))\n",
    "  model.add(Embedding(n_data_samples, 32, input_length=review_length))\n",
    "  model.add(LSTM(100))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "# Deprecated because why not deprecate something that works?\n",
    "#from keras.utils.layer_utils import count_params   # REF (Trainable params): https://stackoverflow.com/questions/45046525/how-can-i-get-the-number-of-trainable-parameters-of-a-model-in-keras\n",
    "#trainable_count = count_params(model.trainable_weights)\n",
    "#non_trainable_count = count_params(model.non_trainable_weights)\n",
    "\n",
    "# REF (How can I get the number of trainable parameters of a model in Keras?): https://stackoverflow.com/questions/45046525/how-can-i-get-the-number-of-trainable-parameters-of-a-model-in-keras\n",
    "import numpy as np\n",
    "trainable_count, non_trainable_count = 0, 0\n",
    "\n",
    "# Update 2024: Deprecation is wild in Keras\n",
    "#from keras import backend as K\n",
    "#trainable_count = int(np.sum([K.count_params(p) for p in set(model.trainable_weights)]))\n",
    "#non_trainable_count = int(np.sum([K.count_params(p) for p in set(model.non_trainable_weights)]))\n",
    "\n",
    "# Also get_shape() is now deprecated\n",
    "#for p in model.trainable_weights:\n",
    "#  trainable_count += int(np.prod(p.get_shape()))\n",
    "#non_trainable_count = 0\n",
    "#for p in model.non_trainable_weights:\n",
    "#  non_trainable_count += int(np.prod(p.get_shape()))\n",
    "\n",
    "# Using num_elements(), not available in Kaggle?\n",
    "#for p in model.trainable_weights:\n",
    "#  trainable_count += int(p.size())  # num_elements() replaced get_shape()\n",
    "#non_trainable_count = 0\n",
    "#for p in model.non_trainable_weights:\n",
    "#  non_trainable_count += int(p.size())\n",
    "\n",
    "# Update 2024: It works!\n",
    "for p in model.trainable_weights:\n",
    "  trainable_count += int(np.prod(p.shape))\n",
    "for p in model.non_trainable_weights:\n",
    "  non_trainable_count += int(np.prod(p.shape))\n",
    "\n",
    "print(f'\\nTotal parameters of the model: {trainable_count + non_trainable_count:,}')\n",
    "print(f'Trainable parameters of the model: {trainable_count:,}')\n",
    "print(f'Non-trainable parameters of the model: {non_trainable_count:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_history = model.fit(x=x_train, y=y_train, validation_data=(x_test[:5000], y_test[:5000]), epochs=1)   # Validate on 5000 of the total datapoints, only need 1 epoch for this dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_accuracy = model_history.history['accuracy']\n",
    "val_accuracy = model_history.history['val_accuracy']\n",
    "print(f'After 1 Epoch:\\nTrain Accuracy: {train_accuracy}\\nValidation Accuracy: {val_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "predictionset = [0 if prediction < 0.5 else 1 for prediction in model.predict(x_test)]   # Normalise for sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "print(f'\\n{classification_report(y_test, predictionset)}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
